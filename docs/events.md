# Control topic events

## Event format

Control topic events are serialized as Avro. The are a few reasons for this decision. First, we need a serialization format that allows for schema evolution. For example, say workers post data files events to the control topic and the sink is shut down before commit. Then the sink is upgraded to a new version with a new event format. When the sink starts up, the coordinator still needs to be able to read the events in the old format so it can commit those files.

There are different formats that allow for this, however Iceberg already has support for Avro serialization for many of its data structures, such as the data file structure that is generated by its writers. The data files structure includes the path as well as file and column-level statistics. Java or Kryo serialization do not allow for schema evolution well, and JSON would require some type of encoding of the binary data used for the range statistics and encryption key metadata.

A downside to using Avro is that, without a schema registry, we need to send the Avro schema with the event in order to deserialize it, as Avro needs both the write and read schemas to deserialize. The data files Avro schema can change based on the table partitioning, not just from a new version of Iceberg. There could be many variations of the Avro schema, especially considering support for multiple table output, so even with a schema registry it would not make sense to store all of these variations. Another downside to using Avro is it adds complexity to the event data model.

Despite this, the size of Avro serialized data files events will generally be much smaller than the Java or Kryo serialized data as the number of data files in an event increases, even though the schema must be included with the event. Thus using Avro should scale better in terms of event size, which is important to avoid any Kafka caps on message size.

Performance-wise, Avro will be slower in most cases due to the overhead in loading the schema for each event. However, event throughput should be relatively low (one per worker per commit interval) so the overhead should be negligible relative to source message processing.

Iceberg writes the Avro schema with the manifest files, so the data files events are structured similarly to this. The Iceberg Flink sink uses Flink’s type system to serialize the Iceberg write results using Kryo, though these objects are transient so don’t need backwards compatibility.

## Event schema

* Event
  * ID
  * Timestamp
  * Type
  * Connector
  * Payload (one of)
    * Commit request payload
      * Commit ID
    * Commit response payload
      * Commit ID
      * Table name
      * Data files
      * Delete files
    * Commit ready payload
      * Commit ID
      * Source topic partition offsets
    * Commit table payload
      * Commit ID
      * Table name
      * Snapshot ID
      * VTTS
    * Commit complete payload
      * Commit ID
      * VTTS

### Payload types

* Commit request
  * from coordinator to worker
  * to initiate a commit
* Commit response
  * from worker to coordinator
  * contains files written for each table
* Commit ready
  * from worker to coordinator
  * after all responses sent for a commit request
* Commit table
  * from coordinator
  * after data is committed to a table
  * not consumed by the connector
  * can be used to trigger downstream processes
* Commit complete
  * from coordinator
  * after all commits to all tables are complete
  * not consumed by the connector
  * can be used to trigger downstream processes
